{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GloVe Loaded.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from __future__ import division\n",
    "\n",
    "filename = 'data/glove.6B.300d.txt' \n",
    "# (glove data set from: https://nlp.stanford.edu/projects/glove/)\n",
    "\n",
    "\n",
    "def loadGloVe(filename):\n",
    "    vocab = []\n",
    "    embd = []\n",
    "    file = open(filename,'r')\n",
    "    for line in file.readlines():\n",
    "        row = line.strip().split(' ')\n",
    "        vocab.append(row[0])\n",
    "        embd.append(row[1:])\n",
    "    print('GloVe Loaded.')\n",
    "    file.close()\n",
    "    return vocab,embd\n",
    "\n",
    "# Pre-trained GloVe embedding\n",
    "vocab,embd = loadGloVe(filename)\n",
    "\n",
    "embedding = np.asarray(embd)\n",
    "embedding = embedding.astype(np.float32)\n",
    "\n",
    "word_vec_dim = len(embd[0]) # word_vec_dim = dimension of each word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "def get_synonymn(word):\n",
    "    synonymns =  wn.synsets(word)[0].lemma_names()\n",
    "    for name in synonymns:\n",
    "        if name != word:\n",
    "            return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import nltk as nlp\n",
    "from nltk import word_tokenize\n",
    "import string\n",
    "import re\n",
    "\n",
    "\n",
    "summaries = []\n",
    "texts = []\n",
    "\n",
    "with open('data/data.csv', 'r',encoding=\"ISO-8859-1\") as csvfile: \n",
    "    data = csv.DictReader(csvfile)\n",
    "    for row in data:\n",
    "        clean_text = row['Text'] \n",
    "        clean_summary =  row['Summaries'] \n",
    "        summaries.append(word_tokenize(clean_summary))\n",
    "        texts.append(word_tokenize(clean_text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAMPLE CLEANED & TOKENIZED TEXT: \n",
      "\n",
      "['Continental', \"'may\", 'run', 'out', 'of', 'cash', \"'\"]\n",
      "\n",
      "SAMPLE CLEANED & TOKENIZED SUMMARY: \n",
      "\n",
      "['Shares', 'in', 'Continental', 'Airlines', 'have', 'tumbled', 'after', 'the', 'firm', 'warned', 'it', 'could', 'run', 'out', 'of', 'cash.In', 'a', 'filing', 'to', 'US', 'regulators', 'the', 'airline', 'warned', 'of', '``', 'inadequate', 'liquidity', \"''\", 'if', 'it', 'fails', 'to', 'reduce', 'wage', 'costs', 'by', '$', '500m', 'by', 'the', 'end', 'of', 'February', '.', 'Continental', 'also', 'said', 'that', ',', 'if', 'it', 'did', 'not', 'make', 'any', 'cuts', ',', 'it', 'expects', 'to', 'lose', '``', 'hundreds', 'of', 'millions', 'of', 'dollars', \"''\", 'in', '2005', 'in', 'current', 'market', 'conditions', '.', 'Failure', 'to', 'make', 'cutbacks', 'may', 'also', 'push', 'it', 'to', 'reduce', 'its', 'fleet', ',', 'the', 'group', 'said', '.', 'Shares', 'in', 'the', 'fifth', 'biggest', 'US', 'carrier', 'had', 'fallen', '6.87', '%', 'on', 'the', 'news', 'to', '$', '10.44', 'by', '1830', 'GMT', '.', '``', 'Without', 'the', 'reduction', 'in', 'wage', 'and', 'benefit', 'costs', 'and', 'a', 'reasonable', 'prospect', 'of', 'future', 'profitability', ',', 'we', 'believe', 'that', 'our', 'ability', 'to', 'raise', 'additional', 'money', 'through', 'financings', 'would', 'be', 'uncertain', ',', \"''\", 'Continental', 'said', 'in', 'its', 'filing', 'to', 'the', 'US', 'Securities', 'and', 'Exchange', 'Commission', '(', 'SEC', ')', '.Airlines', 'have', 'faced', 'tough', 'conditions', 'in', 'recent', 'years', ',', 'amid', 'terrorism', 'fears', 'since', 'the', '11', 'September', 'World', 'Trade', 'Centre', 'attack', 'in', '2001', '.', 'But', 'despite', 'passengers', 'returning', 'to', 'the', 'skies', ',', 'record-high', 'fuel', 'costs', 'and', 'fare', 'wars', 'prompted', 'by', 'competition', 'from', 'low', 'cost', 'carriers', 'have', 'taken', 'their', 'toll', '.', 'Houston-based', 'Continental', 'now', 'has', 'debt', 'and', 'pension', 'payments', 'of', 'nearly', '$', '984m', 'which', 'it', 'must', 'pay', 'off', 'this', 'year', '.', 'The', 'company', 'has', 'been', 'working', 'to', 'streamline', 'its', 'operations', '-', 'and', 'has', 'managed', 'to', 'save', '$', '1.1bn', 'in', 'costs', 'without', 'cutting', 'jobs', '.', 'Two', 'weeks', \"'\", 'ago', 'the', 'group', 'also', 'announced', 'it', 'would', 'be', 'able', 'to', 'shave', 'a', 'further', '$', '48m', 'a', 'year', 'from', 'its', 'costs', 'with', 'changes', 'to', 'wage', 'and', 'benefits', 'for', 'most', 'of', 'its', 'US-based', 'management', 'and', 'clerical', 'staff', '.']\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "index = random.randint(0,len(texts)-1)\n",
    "\n",
    "print(\"SAMPLE CLEANED & TOKENIZED TEXT: \\n\\n\"+str(texts[index]))\n",
    "print(\"\\nSAMPLE CLEANED & TOKENIZED SUMMARY: \\n\\n\"+str(summaries[index])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def np_nearest_neighbour(x):\n",
    "    #returns array in embedding that's most similar (in terms of cosine similarity) to x\n",
    "        \n",
    "    xdoty = np.multiply(embedding,x)\n",
    "    xdoty = np.sum(xdoty,1)\n",
    "    xlen = np.square(x)\n",
    "    xlen = np.sum(xlen,0)\n",
    "    xlen = np.sqrt(xlen)\n",
    "    ylen = np.square(embedding)\n",
    "    ylen = np.sum(ylen,1)\n",
    "    ylen = np.sqrt(ylen)\n",
    "    xlenylen = np.multiply(xlen,ylen)\n",
    "    cosine_similarities = np.divide(xdoty,xlenylen)\n",
    "\n",
    "    return embedding[np.argmax(cosine_similarities)]\n",
    "    \n",
    "\n",
    "\n",
    "def word2vec(word):  # converts a given word into its vector representation\n",
    "    if word in vocab:\n",
    "        return embedding[vocab.index(word)]\n",
    "    else:\n",
    "        try:\n",
    "            synonymn = get_synonymn(word)\n",
    "            if synonymn in vocab:\n",
    "                return embedding[vocab.index(synonymn)]\n",
    "            else:\n",
    "                return embedding[vocab.index('unk')]\n",
    "        except:\n",
    "            return embedding[vocab.index('unk')]\n",
    "\n",
    "def vec2word(vec):   # converts a given vector representation into the represented word \n",
    "    for x in xrange(0, len(embedding)):\n",
    "            if np.array_equal(embedding[x],np.asarray(vec)):\n",
    "                return vocab[x]\n",
    "    return vec2word(np_nearest_neighbour(np.asarray(vec)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector representation of 'unk':\n",
      "\n",
      "[  3.00709993e-01  -4.68670011e-01  -2.06169993e-01  -8.09780002e-01\n",
      "  -2.38890007e-01   2.43290007e-01   1.65379997e-02  -3.56869996e-02\n",
      "  -2.23059997e-01   9.51889992e-01  -3.22730005e-01   2.19799995e-01\n",
      "  -6.75240010e-02  -3.72200012e-01  -3.97179991e-01  -4.38609987e-01\n",
      "   1.19670004e-01  -2.99640000e-01   2.84369998e-02  -8.75440016e-02\n",
      "   1.65690005e-01  -4.94509995e-01  -6.20109975e-01  -1.65739998e-01\n",
      "  -9.72179994e-02  -9.94739980e-02  -8.03069994e-02  -3.93379986e-01\n",
      "  -2.41950005e-01   3.20230007e-01  -5.33200026e-01  -4.01840001e-01\n",
      "  -6.71350002e-01  -7.85610005e-02   5.55459976e-01   2.99970001e-01\n",
      "  -9.96500030e-02  -6.70350015e-01   1.26690000e-01  -1.86179996e-01\n",
      "  -6.26209974e-02   4.52899992e-01   3.92650008e-01   2.41209999e-01\n",
      "  -4.14739996e-01  -6.18900001e-01  -1.04120001e-01  -3.10429990e-01\n",
      "  -6.67880010e-03  -8.32480013e-01   6.51499987e-01   9.01809990e-01\n",
      "   2.41459999e-02  -7.07660019e-02  -3.95799994e-01  -3.64870012e-01\n",
      "  -2.39289999e-01  -1.51449993e-01   2.07770005e-01   5.46710014e-01\n",
      "  -2.50420004e-01  -6.01419985e-01  -5.48200011e-01   7.72489980e-03\n",
      "  -5.32880008e-01   5.03250003e-01  -1.27120003e-01   1.19889997e-01\n",
      "  -6.45839989e-01   3.55760008e-01   1.74960002e-01   1.18380003e-01\n",
      "  -3.21810007e-01   7.48139992e-02  -9.03809965e-02  -2.98429996e-01\n",
      "   1.67980008e-02  -1.27350003e-01   7.35669971e-01  -1.73350006e-01\n",
      "   3.71230006e-01   3.79790008e-01  -5.18010020e-01   2.76210010e-01\n",
      "   2.15120003e-01  -8.25880021e-02   2.16380000e-01   1.25949994e-01\n",
      "   3.84359986e-01  -1.33320004e-01   5.71850017e-02  -2.81269997e-01\n",
      "  -4.43100005e-01   1.34979993e-01   1.33059993e-01  -3.20499986e-02\n",
      "   1.97190002e-01   2.54550010e-01   6.34750009e-01  -2.34740004e-01\n",
      "  -3.60379994e-01   4.11479995e-02  -2.44220003e-01   8.37310016e-01\n",
      "  -2.25040004e-01  -2.96829998e-01   6.38979971e-01  -3.97740006e-01\n",
      "  -1.03220001e-01  -1.74459994e-01  -7.80590028e-02   2.64789999e-01\n",
      "  -4.22500014e-01  -1.06710002e-01  -9.64680016e-02  -1.70269996e-01\n",
      "   2.74969995e-01  -1.28130004e-01   2.47510001e-01   2.59990007e-01\n",
      "   1.83270007e-01   1.09880000e-01   3.54860007e-04  -4.90289986e-01\n",
      "   1.95820004e-01  -4.52259988e-01  -1.36169996e-02   1.07649997e-01\n",
      "  -1.61610004e-02  -2.72419989e-01   7.78770000e-02  -1.18600003e-01\n",
      "   9.27919969e-02  -4.37739998e-01  -2.65390009e-01  -2.65899986e-01\n",
      "   8.05850029e-02   1.86260000e-01   1.73620000e-01  -2.02419996e-01\n",
      "   3.53269994e-01  -6.43350035e-02   1.37639999e-01  -4.44169998e-01\n",
      "   8.75209987e-01  -2.32600003e-01  -6.76569998e-01   2.38910004e-01\n",
      "  -8.01760033e-02   4.95260000e-01  -2.85789996e-01   2.50409991e-01\n",
      "   1.58529997e-01  -1.29600003e-01  -5.15290022e-01  -3.31750005e-01\n",
      "   4.18260008e-01   3.32109988e-01  -1.17929995e+00   2.28180006e-01\n",
      "  -5.77549994e-01   7.73140013e-01   1.60929993e-01   2.33600006e-01\n",
      "  -1.87639996e-01  -2.45159999e-01  -5.48030019e-01   2.31099993e-01\n",
      "  -3.29750001e-01  -1.26460001e-01   3.79839987e-01   3.60060006e-01\n",
      "   6.03820026e-01  -1.58820003e-01  -4.36820000e-01  -6.34440005e-01\n",
      "  -2.88300008e-01  -1.36089996e-01  -2.58210003e-02  -4.07669991e-01\n",
      "   1.86360002e-01  -4.58570004e-01  -2.46110007e-01  -4.58899997e-02\n",
      "   8.76130015e-02  -1.56849995e-01   3.01290005e-01  -8.01760018e-01\n",
      "   1.23630002e-01   7.14580016e-03   1.47510007e-01   3.54710013e-01\n",
      "   1.51199996e-01   8.19379985e-02  -3.67110014e-01  -2.72080004e-01\n",
      "  -3.55969995e-01   1.72069997e-01  -4.18500006e-02   4.95469987e-01\n",
      "  -3.16300005e-01   3.49150002e-01  -3.72949988e-02   2.09959999e-01\n",
      "  -3.01030010e-01  -1.08750001e-01   3.03539991e-01   2.81569988e-01\n",
      "  -7.98799992e-02  -5.06110013e-01  -2.94160008e-01  -3.08609992e-01\n",
      "  -8.24620008e-01  -1.00189999e-01   9.04730037e-02  -1.82380006e-01\n",
      "  -5.92229981e-03   6.38329983e-02   1.52099997e-01  -2.53850013e-01\n",
      "  -6.88310027e-01  -3.45490016e-02   4.51799989e-01   6.22930005e-02\n",
      "  -4.63429987e-01   4.04000014e-01   4.51060012e-02   1.73749998e-01\n",
      "  -2.77449992e-02   3.63609999e-01   7.82350004e-02   1.95380002e-01\n",
      "  -1.65059999e-01   3.96270007e-02  -3.01129997e-01   2.22570002e-01\n",
      "   2.67730001e-02   1.91510007e-01   4.99870002e-01  -3.54909986e-01\n",
      "  -1.99280009e-02   9.07010019e-01  -8.54900002e-01  -3.93610001e-01\n",
      "   4.10299987e-01   1.46310002e-01  -1.56639993e-01   5.39709985e-01\n",
      "   1.48690000e-01  -5.51929995e-02  -4.77180004e-01  -3.74980003e-01\n",
      "  -5.32329977e-01   1.23199999e-01  -1.22270003e-01  -5.81570007e-02\n",
      "   2.42449995e-02   1.25770003e-01  -1.85580000e-01   7.50539973e-02\n",
      "  -7.28219986e-01  -3.58779989e-02   1.59889996e-01  -2.57429987e-01\n",
      "   2.18559995e-01  -2.28100002e-01   4.83560003e-02   5.60230017e-02\n",
      "   1.51109993e-01   2.29450002e-01   4.58460003e-01  -8.70560035e-02\n",
      "  -2.96620011e-01   1.50539996e-02   2.11019993e-01  -3.74459997e-02\n",
      "   7.89020002e-01  -3.41930002e-01  -8.24299991e-01  -7.17480004e-01\n",
      "  -1.96490005e-01   8.75699967e-02  -2.05520004e-01   1.46100000e-02\n",
      "  -3.80879998e-01   6.53090000e-01  -2.05610007e-01  -2.84269992e-02\n",
      "   1.05769997e-02   8.84099957e-03   1.19920000e-01   1.46109998e-01\n",
      "   1.60339996e-01   7.24309981e-02  -4.37599987e-01  -2.59790003e-01\n",
      "   5.81579983e-01   4.92670000e-01  -1.12760000e-01  -2.77749985e-01]\n"
     ]
    }
   ],
   "source": [
    "word = \"unk\"\n",
    "print(\"Vector representation of '\"+str(word)+\"':\\n\") \n",
    "print(word2vec(word)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#REDUCE DATA (FOR SPEEDING UP THE NEXT STEPS)\n",
    "\n",
    "MAXIMUM_DATA_NUM = 50000\n",
    "\n",
    "texts = texts[0:MAXIMUM_DATA_NUM]\n",
    "summaries = summaries[0:MAXIMUM_DATA_NUM]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_limit = []\n",
    "embd_limit = []\n",
    "\n",
    "i=0\n",
    "for text in texts:\n",
    "    for word in text:\n",
    "        if word not in vocab_limit:\n",
    "            if word in vocab:\n",
    "                vocab_limit.append(word)\n",
    "                embd_limit.append(word2vec(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for summary in summaries:\n",
    "    for word in summary:\n",
    "        if word not in vocab_limit:\n",
    "            if word in vocab:\n",
    "                vocab_limit.append(word)\n",
    "                embd_limit.append(word2vec(word))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if 'eos' not in vocab_limit:\n",
    "    vocab_limit.append('eos')\n",
    "    embd_limit.append(word2vec('eos'))\n",
    "if 'unk' not in vocab_limit:\n",
    "    vocab_limit.append('unk')\n",
    "    embd_limit.append(word2vec('unk'))\n",
    "\n",
    "null_vector = np.zeros([word_vec_dim])\n",
    "\n",
    "vocab_limit.append('<PAD>')\n",
    "embd_limit.append(null_vector)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vec_summaries = []\n",
    "\n",
    "for summary in summaries:\n",
    "    \n",
    "    vec_summary = []\n",
    "    \n",
    "    for word in summary:\n",
    "        vec_summary.append(word2vec(word))\n",
    "            \n",
    "    vec_summary.append(word2vec('eos'))\n",
    "    \n",
    "    vec_summary = np.asarray(vec_summary)\n",
    "    vec_summary = vec_summary.astype(np.float32)\n",
    "    \n",
    "    vec_summaries.append(vec_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vec_texts = []\n",
    "\n",
    "for text in texts:\n",
    "    \n",
    "    vec_text = []\n",
    "    \n",
    "    for word in text:\n",
    "        vec_text.append(word2vec(word))\n",
    "    \n",
    "    vec_text = np.asarray(vec_text)\n",
    "    vec_text = vec_text.astype(np.float32)\n",
    "    \n",
    "    vec_texts.append(vec_text)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Saving processed data in another file.\n",
    "\n",
    "import pickle\n",
    "with open('vocab_limit', 'wb') as fp:\n",
    "    pickle.dump(vocab_limit, fp)\n",
    "with open('embd_limit', 'wb') as fp:\n",
    "    pickle.dump(embd_limit, fp)\n",
    "with open('vec_summaries', 'wb') as fp:\n",
    "    pickle.dump(vec_summaries, fp)\n",
    "with open('vec_texts', 'wb') as fp:\n",
    "    pickle.dump(vec_texts, fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
