{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Importations\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import helpers #For formatting and generating random sequence data\n",
    "import os\n",
    "import pickle \n",
    "\n",
    "tf.reset_default_graph() # kind of like clearing cache\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.2.1'"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Define some variables\n",
    "#Padding is necessary to handle variability of sentence lengths\n",
    "PAD = 0\n",
    "EOS = 1 #End of sentence\n",
    "\n",
    "vocab_size = 10 #max length for input sequence\n",
    "input_embedding_size = 100 #The vector  length\n",
    "\n",
    "#Hidden units - How big is the encoder and how big is the decoder?\n",
    "encoder_hidden_units = 20\n",
    "decoder_hidden_units = encoder_hidden_units *2 #The decoder in this case is twice the size of the encoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Placeholders - Gateways for data into our computation graph\n",
    "encoder_inputs = tf.placeholder(shape=(None, None), dtype=tf.int32, name='encoder_inputs')\n",
    "#contains the lengths for each of the sequence in the batch, we will pad so all the same\n",
    "#if you don't want to pad, check out dynamic memory networks to input variable length sequences\n",
    "encoder_inputs_length = tf.placeholder(shape=(None,), dtype=tf.int32, name='encoder_inputs_length')\n",
    "decoder_targets = tf.placeholder(shape=(None, None), dtype=tf.int32, name='decoder_targets')            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# read word embedding from Glove\n",
    "# with open(os.path.join('data', '{}.pkl'.format('vocabulary-embedding')), 'rb') as fp:\n",
    "#     em, idx2word, word2idx, glove_idx2idx = pickle.load(fp)\n",
    "# vocab_size, embedding_size = em.shape\n",
    "\n",
    "# with open(os.path.join('data', '{}.data.pkl'.format('vocabulary-embedding')), 'rb') as fp:\n",
    "#     X, Y = pickle.load(fp)\n",
    "    \n",
    "# embed = tf.Variable(em,dtype=tf.float32) \n",
    "\n",
    "# #embed = tf.Variable(tf.random_uniform([vocab_size, input_embedding_size],  -1.0, 1.0),dtype=tf.float32) \n",
    "\n",
    "# embeddings = em\n",
    "# embed.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Embeddings - Initialize an embedding matrix randomly -Interval from -1 to 1 of values\n",
    "embeddings = tf.Variable(tf.random_uniform([vocab_size, input_embedding_size],  -1.0, 1.0),dtype=tf.float32) \n",
    "\n",
    "#Put encoder inputs into our embedding matrix. \n",
    "encoder_inputs_embedded = tf.nn.embedding_lookup(embeddings,encoder_inputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.ops.rnn_cell import LSTMCell,LSTMStateTuple\n",
    "\n",
    "# We initialize the LSTMCell with number of neurons\n",
    "# Each of it are LSTM\n",
    "encoder_cell = LSTMCell(encoder_hidden_units) \n",
    "encoder_cell_backward = LSTMCell(encoder_hidden_units)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# #Bi-directional RNN that takes into consideration both the past and the future\n",
    "\n",
    "# Final Hidden State of the encoder \n",
    "((encoder_fw_outputs, encoder_bw_outputs), \n",
    " (encoder_fw_final_state, encoder_bw_final_state)) = (\n",
    "    tf.nn.bidirectional_dynamic_rnn(cell_fw=encoder_cell,\n",
    "                                    cell_bw=encoder_cell_backward,\n",
    "                                    inputs=encoder_inputs_embedded,\n",
    "                                    sequence_length=encoder_inputs_length,\n",
    "                                    dtype=tf.float32, \n",
    "                                    time_major=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'bidirectional_rnn/fw/fw/TensorArrayStack/TensorArrayGatherV3:0' shape=(?, ?, 20) dtype=float32>"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encoder Forward Output\n",
    "encoder_fw_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMStateTuple(c=<tf.Tensor 'bidirectional_rnn/fw/fw/while/Exit_2:0' shape=(?, 20) dtype=float32>, h=<tf.Tensor 'bidirectional_rnn/fw/fw/while/Exit_3:0' shape=(?, 20) dtype=float32>)"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encoder Forward Final State\n",
    "# h and c are commonly used to denote \"output value\" and \"cell state\"\n",
    "encoder_fw_final_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'bidirectional_rnn/fw/fw/while/Exit_3:0' shape=(?, 20) dtype=float32>"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_fw_final_state.h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Bidirectional step\n",
    "\n",
    "# In this case we will not discard outputs, they would be used for attention.\n",
    "\n",
    "# Get the encoder output (forward and backward) and concatenate it along\n",
    "# Bidirectional RNN steps\n",
    "\n",
    "encoder_outputs = tf.concat((encoder_fw_outputs, encoder_bw_outputs), 2)\n",
    "\n",
    "# http://colah.github.io/posts/2015-08-Understanding-LSTMs/   \n",
    "# Get the final cell state of the encoder by concatenate\n",
    "# the encoder forward final state and backward\n",
    "# h and c are commonly used to denote \"output value\" and \"cell state\"\n",
    "# Those tensors represent combined internal state of the cell, and should be passed together\n",
    "encoder_final_state_c = tf.concat(\n",
    "    (encoder_fw_final_state.c, encoder_bw_final_state.c), 1)\n",
    "\n",
    "encoder_final_state_h = tf.concat(\n",
    "    (encoder_fw_final_state.h, encoder_bw_final_state.h), 1)\n",
    "\n",
    "# TF Tuple used by LSTM Cells for state_size, zero_state, and output state\n",
    "# This value of encoder final state will be used to feed the decoder\n",
    "encoder_final_state = LSTMStateTuple(\n",
    "    c=encoder_final_state_c,\n",
    "    h=encoder_final_state_h\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#defining decoder\n",
    "decoder_cell = LSTMCell(decoder_hidden_units)\n",
    "decoder_cell_backward = LSTMCell(decoder_hidden_units)\n",
    "\n",
    "# The encoder will be feed to the input in batches\n",
    "encoder_max_time, batch_size = tf.unstack(tf.shape(encoder_inputs))\n",
    "\n",
    "# Decoder length, +3 (make it a little bigger) because EOS token \n",
    "decoder_lengths = encoder_inputs_length + 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Output projection\n",
    "\n",
    "#Define our weights and biases\n",
    "W = tf.Variable(tf.random_uniform([decoder_hidden_units],vocab_size, -1.0,1.0), dtype = tf.float32)\n",
    "b = tf.Variable(tf.zeros([vocab_size]),dtype=tf.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'unstack:1' shape=() dtype=int32>"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check on the batch size\n",
    "batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#create padded inputs for the decoder from the word embeddings\n",
    "\n",
    "#were telling the program to test a condition, and trigger an error if the condition is false.\n",
    "assert EOS == 1 and PAD == 0\n",
    "\n",
    "# EOS value is 1, and PAD value is 0\n",
    "eos_time_slice = tf.ones([batch_size], dtype=tf.int32, name='EOS')\n",
    "pad_time_slice = tf.zeros([batch_size], dtype=tf.int32, name='PAD')\n",
    "\n",
    "#retrieves rows of the params tensor. The behavior is similar to using \n",
    "#indexing with arrays in numpy\n",
    "# Adding the padding and EOS to the embeddings\n",
    "eos_step_embedded = tf.nn.embedding_lookup(embeddings, eos_time_slice)\n",
    "pad_step_embedded = tf.nn.embedding_lookup(embeddings, pad_time_slice)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Here instead of using TensorFlow bidirectional function\n",
    "# We manually do the loop function through time\n",
    "# This is to get the initial cell state and input to RNN\n",
    "# Usually we will use dynamic_rnn function for this, but for the sake \n",
    "#    of knowledge, lets do this\n",
    "\n",
    "# This function done nothing except initialization of the values\n",
    "\n",
    "def loop_fn_initial():\n",
    "    initial_elements_finished = (0 >= decoder_lengths)  # all False at the initial step\n",
    "    #end of sentence\n",
    "    initial_input = eos_step_embedded\n",
    "    #last time steps cell state\n",
    "    initial_cell_state = encoder_final_state\n",
    "    #none\n",
    "    initial_cell_output = None\n",
    "    #none\n",
    "    initial_loop_state = None  # we don't need to pass any additional information\n",
    "    \n",
    "    return (initial_elements_finished,\n",
    "            initial_input,\n",
    "            initial_cell_state,\n",
    "            initial_cell_output,\n",
    "            initial_loop_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#attention mechanism --choose which previously generated token to pass as input in the next timestep\n",
    "def loop_fn_transition(time, previous_output, previous_state, previous_loop_state):\n",
    "\n",
    "    \n",
    "    def get_next_input():\n",
    "        #dot product between previous ouput and weights, then + biases\n",
    "        this_weights = tf.cast(tf.expand_dims(W,1), tf.float32)\n",
    "        output_logits = tf.add(tf.matmul(previous_output,tf.expand_dims(W,1) ), b)\n",
    "        \n",
    "\n",
    "        #Logits simply means that the function operates on the unscaled output of \n",
    "        #earlier layers and that the relative scale to understand the units is linear. \n",
    "        #It means, in particular, the sum of the inputs may not equal 1, that the values are not probabilities \n",
    "        #(you might have an input of 5).\n",
    "        #prediction value at current time step\n",
    "        \n",
    "        #Returns the index with the largest value across axes of a tensor.\n",
    "        prediction = tf.argmax(output_logits, axis=1)\n",
    "        #embed prediction for the next input\n",
    "        next_input = tf.nn.embedding_lookup(embeddings, prediction)\n",
    "        \n",
    "        return next_input\n",
    "    \n",
    "    elements_finished = (time >= decoder_lengths) # this operation produces boolean tensor of [batch_size]\n",
    "                                                  # defining if corresponding sequence has ended\n",
    "    \n",
    "    \n",
    "    \n",
    "    #Computes the \"logical and\" of elements across dimensions of a tensor.\n",
    "    finished = tf.reduce_all(elements_finished) # -> boolean scalar\n",
    "    #Return either fn1() or fn2() based on the boolean predicate pred.\n",
    "    inputm = tf.cond(finished, lambda: pad_step_embedded, get_next_input)\n",
    "    \n",
    "    #set previous to current\n",
    "    state = previous_state\n",
    "    output = previous_output\n",
    "    loop_state = None\n",
    "    return (elements_finished, \n",
    "            inputm,\n",
    "            state,\n",
    "            output,\n",
    "            loop_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def loop_fn(time, previous_output, previous_state, previous_loop_state):\n",
    "    print('ddDD')\n",
    "    if previous_state is None:    # time == 0\n",
    "        assert previous_output is None and previous_state is None\n",
    "        print('aaa')\n",
    "        \n",
    "        return loop_fn_initial()\n",
    "    \n",
    "    else:\n",
    "        \n",
    "\n",
    "        return loop_fn_transition(time, previous_output, previous_state, previous_loop_state)\n",
    "\n",
    "#Creates an RNN specified by RNNCell cell and loop function loop_fn.\n",
    "#This function is a more primitive version of dynamic_rnn that provides more direct access to the \n",
    "#inputs each iteration. It also provides more control over when to start and finish reading the sequence, \n",
    "#and what to emit for the output.\n",
    "#ta = tensor array\n",
    "# decoder_outputs_ta, decoder_final_state, _ = tf.nn.raw_rnn(decoder_cell,loop_fn)\n",
    "# decoder_outputs = decoder_outputs_ta.stack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ddDD\n",
      "aaa\n",
      "ddDD\n",
      "jsshsa\n",
      "jsshsa\n"
     ]
    }
   ],
   "source": [
    "decoder_outputs_ta, decoder_final_state, _ = tf.nn.raw_rnn(decoder_cell,loop_fn)\n",
    "decoder_outputs = decoder_outputs_ta.stack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'TensorArrayStack/TensorArrayGatherV3:0' shape=(?, ?, 40) dtype=float32>"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# to convert output to human readable prediction\n",
    "#we will reshape output tensor\n",
    "\n",
    "#Unpacks the given dimension of a rank-R tensor into rank-(R-1) tensors.\n",
    "#reduces dimensionality\n",
    "decoder_max_steps, decoder_batch_size, decoder_dim = tf.unstack(tf.shape(decoder_outputs))\n",
    "#flettened output tensor\n",
    "decoder_outputs_flat = tf.reshape(decoder_outputs, (-1, decoder_dim))\n",
    "#pass flattened tensor through decoder\n",
    "decoder_logits_flat = tf.add(tf.matmul(decoder_outputs_flat, tf.expand_dims(W,1)), b)\n",
    "\n",
    "#prediction vals\n",
    "decoder_logits = tf.reshape(decoder_logits_flat, (decoder_max_steps, decoder_batch_size, vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#final prediction\n",
    "decoder_prediction = tf.argmax(decoder_logits, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#cross entropy loss\n",
    "#one hot encode the target values so we don't rank just differentiate\n",
    "stepwise_cross_entropy = tf.nn.softmax_cross_entropy_with_logits(\n",
    "    labels=tf.one_hot(decoder_targets, depth=vocab_size, dtype=tf.float32),\n",
    "    logits=decoder_logits,\n",
    ")\n",
    "\n",
    "#loss function\n",
    "loss = tf.reduce_mean(stepwise_cross_entropy)\n",
    "#train it \n",
    "train_op = tf.train.AdamOptimizer().minimize(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of examples 510 510\n",
      "dimension of embedding space for words 100\n",
      "vocabulary size 40000 the last 10 words can be used as place holders for unknown/oov words\n",
      "total number of different words 22464 22464\n",
      "number of words outside vocabulary which we can substitue using glove similarity 0\n",
      "number of words that will be regarded as unknonw(unk)/out-of-vocabulary(oov) -17536\n"
     ]
    }
   ],
   "source": [
    "import  pickle\n",
    "FN0 = 'vocabulary-embedding'\n",
    "nb_unknown_words = 10\n",
    "\n",
    "with open('data/%s.pkl'%FN0, 'rb') as fp:\n",
    "    embedding, idx2word, word2idx, glove_idx2idx = pickle.load(fp)\n",
    "vocab_size, embedding_size = embedding.shape\n",
    "\n",
    "\n",
    "with open('data/%s.data.pkl'%FN0, 'rb') as fp:\n",
    "    X, Y = pickle.load(fp)\n",
    "\n",
    "    \n",
    "print('number of examples',len(X),len(Y))\n",
    "print('dimension of embedding space for words',embedding_size)\n",
    "print('vocabulary size', vocab_size, 'the last %d words can be used as place holders for unknown/oov words'%nb_unknown_words)\n",
    "print('total number of different words',len(idx2word), len(word2idx))\n",
    "print('number of words outside vocabulary which we can substitue using glove similarity', len(glove_idx2idx))\n",
    "print('number of words that will be regarded as unknonw(unk)/out-of-vocabulary(oov)',len(idx2word)-vocab_size-len(glove_idx2idx))\n",
    "\n",
    "\n",
    "nb_unknown_words = 10\n",
    "for i in range(nb_unknown_words):\n",
    "    idx2word[vocab_size-1-i] = '<%d>'%i\n",
    "    \n",
    "    \n",
    "oov0 = vocab_size-nb_unknown_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Lobster Stew'"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle \n",
    "x = pickle.load(open( \"data/tokens3.pkl\",'rb' ))\n",
    "x[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "head of the batch:\n",
      "[8, 3, 2]\n",
      "[9, 5, 4]\n",
      "[9, 8, 4, 4, 7]\n",
      "[5, 2, 6, 6, 6, 2]\n",
      "[6, 5, 2, 9]\n",
      "[5, 7, 7, 3]\n",
      "[5, 5, 3, 8, 4, 2, 3, 2]\n",
      "[5, 3, 6, 9, 7]\n",
      "[9, 8, 4, 9, 8, 5]\n",
      "[9, 7, 2, 5, 3, 8, 5]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "\n",
    "batches = helpers.random_sequences(length_from=3, length_to=8,\n",
    "                                   vocab_lower=2, vocab_upper=10,\n",
    "                                   batch_size=batch_size)\n",
    "\n",
    "print('head of the batch:')\n",
    "for seq in next(batches)[:10]:\n",
    "    print(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def next_feed():\n",
    "    batch = next(batches)\n",
    "    encoder_inputs_, encoder_input_lengths_ = helpers.batch(batch)\n",
    "    decoder_targets_, _ = helpers.batch(\n",
    "        [(sequence) + [EOS] + [PAD] * 2 for sequence in batch]\n",
    "    )\n",
    "    return {\n",
    "        encoder_inputs: encoder_inputs_,\n",
    "        encoder_inputs_length: encoder_input_lengths_,\n",
    "        decoder_targets: decoder_targets_,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loss_track = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0\n",
      "  minibatch loss: 2.3019869327545166\n",
      "  sample 1:\n",
      "    input     > [6 8 4 3 7 8 2 8]\n",
      "    predicted > [0 0 0 0 0 0 0 0 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [4 3 7 6 0 0 0 0]\n",
      "    predicted > [0 0 0 0 0 0 0 0 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [2 9 9 0 0 0 0 0]\n",
      "    predicted > [0 0 0 0 0 0 0 0 0 0 0]\n",
      "\n",
      "batch 1000\n",
      "  minibatch loss: 1.9727472066879272\n",
      "  sample 1:\n",
      "    input     > [9 7 7 0 0 0 0 0]\n",
      "    predicted > [0 0 0 0 0 0 0 0 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [6 9 5 3 2 8 9 7]\n",
      "    predicted > [0 0 0 0 0 0 0 0 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [4 5 4 4 6 4 0 0]\n",
      "    predicted > [0 0 0 0 0 0 0 0 0 0 0]\n",
      "\n",
      "batch 2000\n",
      "  minibatch loss: 1.9871453046798706\n",
      "  sample 1:\n",
      "    input     > [2 9 3 0 0 0 0 0]\n",
      "    predicted > [0 0 0 0 0 0 0 0 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [9 5 2 0 0 0 0 0]\n",
      "    predicted > [0 0 0 0 0 0 0 0 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [9 4 8 2 3 2 0 0]\n",
      "    predicted > [0 0 0 0 0 0 0 0 0 0 0]\n",
      "\n",
      "batch 3000\n",
      "  minibatch loss: 2.026182174682617\n",
      "  sample 1:\n",
      "    input     > [5 5 5 7 4 3 6 0]\n",
      "    predicted > [0 0 0 0 0 0 0 0 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [8 3 3 8 2 9 6 8]\n",
      "    predicted > [0 0 0 0 0 0 0 0 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [5 8 5 5 8 3 0 0]\n",
      "    predicted > [0 0 0 0 0 0 0 0 0 0 0]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "max_batches = 3001\n",
    "batches_in_epoch = 1000\n",
    "\n",
    "try:\n",
    "    for batch in range(max_batches):\n",
    "        fd = next_feed()\n",
    "        _, l = sess.run([train_op, loss], fd)\n",
    "        loss_track.append(l)\n",
    "\n",
    "        if batch == 0 or batch % batches_in_epoch == 0:\n",
    "            print('batch {}'.format(batch))\n",
    "            print('  minibatch loss: {}'.format(sess.run(loss, fd)))\n",
    "            predict_ = sess.run(decoder_prediction, fd)\n",
    "            for i, (inp, pred) in enumerate(zip(fd[encoder_inputs].T, predict_.T)):\n",
    "                print('  sample {}:'.format(i + 1))\n",
    "                print('    input     > {}'.format(inp))\n",
    "                print('    predicted > {}'.format(pred))\n",
    "                if i >= 2:\n",
    "                    break\n",
    "            print()\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print('training interrupted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 2.0262 after 300100 examples (batch_size=100)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x122651fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(loss_track)\n",
    "print('loss {:.4f} after {} examples (batch_size={})'.format(loss_track[-1], len(loss_track)*batch_size, batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
